<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux service and its auto start-up]]></title>
    <url>%2F2018%2F12%2F14%2Flinux-service-and-its-auto-start-up%2F</url>
    <content type="text"><![CDATA[添加到service很简单，就是service会到 /etc/init.d 下寻找可执行脚本，然后运行该脚本。以我新添加的service elasticsearch start 为例：在 /etc/init.d 下添加 elasticsearch 脚本，权限 755脚本内容如下： 12345678910111213141516171819202122232425262728#!/bin/bash#chkconfig:2345 80 05 --指定在哪几个级别执行。80为启动的优先级，05为关闭的优先级。启动与关闭都是，数字小的优先执行#description: elasticsearch start-upRETVAL=0start()&#123; su - es -c "cd elasticsearch-6.3.2/bin &amp;&amp; ./elasticsearch -d" echo '[start]'&#125;stop()&#123; espid=`lsof -i:9200|sed '1d'|awk '&#123;print $2&#125;' | sed -n '1p'` kill -9 $espid echo '[stop]'&#125;case $1 in start) start ;; stop) stop ;; restart) stop start ;;esacexit $RETVAL service的添加是即时生效的，不需任何重启，这样就可以了 开机启动chkconfig是用于把服务加到开机自动启动列表里12chkconfig --add elasticsearch chkconfig elasticsearch on 或 off 然后在 etc/rc.d/rc2 3 4 5.d 下就会看到 S80elasticsearch，添加到自启动成功S80即其启动的优先级，关闭是K05 [kill]。启动与关闭都是，数字小的优先执行。 附：Linux 的7个运行级别运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 所以一般都是2345… 脚本中的几个Linux命令su - es -c “xxxx” su es，并且采用es用户的环境变量，来执行后边的命令 “xxxx” 这条命令对自动执行各种不能以root执行的程序，实在是太实用了 awk awk 是行处理工具，是一行一行地对行内的内容处理，例如修改每行中的一些字符 awk对一行处理时，对行内元素，默认以空格为分割符，也可以自定义分割符 sed sed 也是行处理工具，它是一次处理一行，例如删除某行 sed 的参数与vim命令类似，例如 ‘1d’ 删除第一行]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es systematic design]]></title>
    <url>%2F2018%2F12%2F01%2Fes-systematic-design%2F</url>
    <content type="text"><![CDATA[back up a es design1234567891011121314151617181920212223242526272829303132PUT /dw_table&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot;, &quot;filter&quot;:[&quot;lowercase&quot;,&quot;my_stopwords&quot;] &#125;, &quot;my_not_analyzed_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_not_analyzed_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;pattern&quot;, &quot;pattern&quot;: &quot;_| &quot; &#125;, &quot;my_not_analyzed_tokenizer&quot;:&#123; &quot;type&quot;: &quot;pattern&quot;, &quot;pattern&quot;: &quot;this_is_a_pattern_that_your_query_str_cannot_contain&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;:&#123; &quot;type&quot;:&quot;stop&quot;, &quot;stopwords&quot;:[&quot;base&quot;,&quot;report&quot;] &#125; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354PUT /dw_table/_mapping/awesome_table&#123; &quot;properties&quot;: &#123; &quot;table_name&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125;, &quot;describe&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;charge&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_not_analyzed_analyzer&quot;, &quot;search_analyzer&quot;: &quot;my_analyzer&quot; &#125;, &quot;demand&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_not_analyzed_analyzer&quot;, &quot;search_analyzer&quot;: &quot;my_analyzer&quot; &#125;, &quot;detail&quot;: &#123; &quot;properties&quot;: &#123; &quot;col_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;:&quot;my_analyzer&quot; &#125;, &quot;data_type&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125;, &quot;describe&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;example&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125;, &quot;partition&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125; &#125;, &quot;detail_describe&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125; 象征性地插入一条数据123456789101112131415161718192021222324252627282930313233343536373839PUT /dw_table/awesome_table/1&#123; &quot;table_name&quot;: &quot;base_m_searchid_to_keyword&quot;, &quot;detail&quot;: [ &#123; &quot;describe&quot;: &quot;分区字段&quot;, &quot;partition&quot;: &quot;partition_col&quot;, &quot;col_name&quot;: &quot;date&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;, &#123; &quot;describe&quot;: &quot;是否包含敏感词&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;is_spam&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;int&quot; &#125;, &#123; &quot;describe&quot;: &quot;搜索词&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;keyword&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;, &#123; &quot;describe&quot;: &quot;一次搜索的唯一编号&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;searchid&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125; ], &quot;charge&quot;: &quot;cindy&quot;, &quot;describe&quot;: &quot;searchid对应的关键词相关信息&quot;, &quot;demand&quot;: &quot;cindy&quot;, &quot;detail_describe&quot;: &quot;searchid和搜索词的关联表&quot;, &quot;v&quot;: 1&#125; 只有es分析器和mapping是不够的，还要有后端针对不同的输入字符串而定制的查询方式。查询设计一：例如以下query_str 是完整字符串，str_en 就是输入字符中的英文字符，str_ch是中文字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950query = &#123; &quot;from&quot;: 0,&quot;size&quot;:20, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;table_name&quot;: &#123; &quot;query&quot;: str_en, &quot;boost&quot;: 5 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;describe&quot;: &#123; &quot;query&quot;: str_ch, &quot;boost&quot;: 2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;charge&quot;: query_str &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;demand&quot;: query_str &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;detail.col_name&quot;: str_en &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;detail.describe&quot;: str_ch &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;detail.example&quot;: query_str &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es -- not_analyzed]]></title>
    <url>%2F2018%2F11%2F08%2Fes-not_analyzed%2F</url>
    <content type="text"><![CDATA[es版本6.4 使某field建立索引时不分词，但是搜索时分词 很多教程中说，在mapping中设置field不分词为 &quot;index&quot;:&quot;not_analyzed&quot;，但是版本更新，这个语法已经错误了，&quot;index&quot;的值只能是true和false，当值为false时，是表示这个field不被索引，而不是不analyze。去查询一个&quot;index&quot;:false的field，会报错400错误如下：1&quot;reason&quot;: &quot;Cannot search on field [demand] since it is not indexed.&quot; 那该如何改？见以下链接： https://www.elastic.co/blog/strings-are-dead-long-live-strings 就是 &quot;type&quot;: &quot;keyword&quot; 代替了 &quot;not_analyzed&quot; 但有个问题是，把一个field设了&quot;type&quot;: &quot;keyword&quot;后，不能再设置它的 &quot;search_analyzer&quot;，即123456789PUT /dw/_mapping/test&#123; &quot;properties&quot;: &#123; &quot;field_1&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;search_analyzer&quot;:&quot;standard&quot; &#125; &#125;&#125; 这样是会报错的：1&quot;reason&quot;: &quot;Mapping definition for [col1] has unsupported parameters: [search_analyzer : standard]&quot; 这就导致了&quot;type&quot;: &quot;keyword&quot;的局限性，具体对比如下 1234567891011121314151617PUT /dw/_mapping/test&#123; &quot;properties&quot;: &#123; &quot;field_1&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;field_2&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125;PUT /dw/test/1&#123; &quot;field_1&quot;:&quot;hello world&quot;, &quot;field_2&quot;:&quot;hello world&quot;&#125; match / “hello” “hello world” “hello world kidult” “field_1” × ✔️ × “field_2” ✔️ ✔️ ✔️ 我希望表格中右上角那个 × 是 ✔️至此，已经明了了。为了实现需求，我的解决方法是：对于field_1，还是用&quot;type&quot;: &quot;text&quot;，然后analyzer用一个自定义的fake_analyzer——它来实现不分词search_analyzer用另一个analyzer，例如&quot;standard&quot; ps:上边已经说清楚”keyword”了，不过写都写到这儿了，就顺便记一下term，term把整个query当做一个词，运行如下： term / “hello” “hello world” “hello world kidult” “field_1” × ✔️ × “field_2” ✔️ × ×]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming exercise]]></title>
    <url>%2F2018%2F10%2F30%2FDynamic-Programming-exercise%2F</url>
    <content type="text"><![CDATA[Those who cannt remember the past are condemned to repeat it把问题划分为重复性的子问题，并记住计算过的部分来避免重复计算 STEPS Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution. Construct an optimal solution from computed information. 近来闲暇，看看dp做做简单练习。摘要第一句引自博客，STEPS引自算法导论，感觉别人总结的真是到位 先边看导论，边做小练习，做多了再自行归纳 上台阶每次可以上一个或两个台阶，上到n级台阶，问共有多少种上法稍微分析便发现其实是个 斐波那契数列12345678910111213141516171819202122# 自顶向下def upstairs(n): s=&#123;1:1,2:2&#125; if n in s.keys(): return s[n] for i in range(3,n+1): s[i] = s[i-1]+s[i-2] return s[n]# 自底向上s=&#123;1:1,2:2&#125;def upstairs2(n): if n in s.keys(): return s[n] s[n]=upstairs2(n-1)+upstairs2(n-2) return s[n]p=upstairs(10)q=upstairs2(10)print(p,q) 硬币找零假设有三种类型的硬币1，3，5，要找零一定数额n,问最少要多少枚硬币。12345678910111213141516171819202122# 带备忘录自顶向下s = &#123;1:1,2:2,3:1,4:2,5:1&#125;def coins(n): if n in s.keys(): return s[n] s[n] = min(coins(n-1),coins(n-3),coins(n-5))+1 return s[n]# 自底向上def coins2(n): s = &#123;1: 1, 2: 2, 3: 1, 4: 2, 5: 1&#125; if n in s.keys(): return s[n] for i in range(6,n+1): s[i] = min(s[i - 1], s[i - 3], s[i - 5]) + 1 return s[n]x=coins(154)y=coins2(154)print(x,y) 切钢条算法导论书中原例：不同长度的钢条对应着不同的售价，对于长度为n的钢条，问最大售价是多少12345678910111213141516171819202122232425262728293031323334353637383940p=&#123;1:1,2:5,3:8,4:9,5:10,6:17&#125;# 硬核循环def rod_cut1(n): max_price = 0 for i in range(1,n+1): tmp_max = p[i]+rod_cut1(n-i) if tmp_max&gt;max_price: max_price = tmp_max return max_price# 自顶向下r=&#123;&#125;def rod_cut2(n): if n in r.keys(): return r[n] r[n] = 0 for i in range(1,n+1): tmp_max = p[i]+rod_cut2(n-i) if tmp_max&gt;r[n]: r[n] = tmp_max return r[n]# 自底向上def rod_cut3(n): r = &#123;0:0&#125; for i in range(1,n+1): a = 0 for j in range(1,i+1): tmp_max = p[j]+r[i-j] if tmp_max&gt;a: a=tmp_max r[i]=a return r[n]x=rod_cut1(5)y=rod_cut2(6)z=rod_cut3(5)print(x,y,z) 捡苹果一个m x n 的格子阵里，每个格子里有一些苹果。从左上角开始，每次只能向右或向下移动一格，每到一个格子就把格子里的苹果捡起来。问移动到特定格子，最多能捡多少个苹果12345678910111213141516171819202122232425apple=[[1,8,3,2],[4,3,6,7],[4,9,8,7]]# 带备忘录自顶向下import numpy as npr = np.zeros((3,4))def max_apple(x,y): if r[x][y] &gt; 0: return r[x][y] if x != 0 and y != 0: r[x][y]=max(max_apple(x-1,y),max_apple(x,y-1))+apple[x][y] elif x==0 and y!=0: r[x][y]=max_apple(x,y-1)+apple[x][y] elif x!=0 and y==0: r[x][y] = max_apple(x-1, y) + apple[x][y] else: r[x][y]=apple[x][y] return r[x][y]x = max_apple(2,2)print(x)# 自底向上]]></content>
      <tags>
        <tag>algorithm</tag>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LOL S8 Log]]></title>
    <url>%2F2018%2F10%2F15%2FLOL-S8-Log%2F</url>
    <content type="text"><![CDATA[入围赛不提了好吧，可以说LPL三号种子EDG无悬念出线 小组赛B组1234RNG 5-2C9 4-2VIT 3-2GEN 1-5 RNG 高光时刻 组内首轮RNG vs GEN。GEN青钢影带线几近带到高地了，ming洛看似一手失败的大招被霞大招躲了，但细心看就会发现这是RNG的战术————我知道你霞有大招，我洛大招cd比你短，这波就是逼你换大招的，我洛连闪现都没用。然后letme一波完美开车，瞎子洛逼霞走位，霞慌忙交出闪现，也躲不过这场车祸。连高地都带掉了的青钢影懵逼着就回家了。GEN还是不能阻止一波暴毙。RNG抓机会打团是真的无敌，瞬间暴毙看着真的爽。 第二轮 RNG 爆锤 GEN。前几级侧漏霸气的小团都不值得提了，最高光莫过于：在红色方蓝buff处，冰女定住刀妹开团，RNG输出差些许刀妹行至二塔处苟活，此时下路一塔还在。瞬间！UZI滑板鞋给锤石大招，从蛤蟆处闪现过墙，然后锤石砸下去击飞三人，让男剑魔开大天神下凡接连击飞，冰女奥拉夫滑板鞋跟上输出，打出一波小团灭。完美combo把GEN心理防线都摧毁了。 状态来了就是神挡杀神，RNG show time!最后一场争第一的加赛C9，因为阵容原因，几乎所有人都认为时间已经到后期，RNG胜算很低了，甚至有人说：单看阵容，不知道右边怎么输。然而，RNG这个名字，100%的胜利象征。太刺激了. 段友·真人才 三星真兄弟，掀翻SKT，拦住格里芬，拖住KZ，咬了C9一口，临走还送了RNG两分，最后连名字都不留下，你们只知道吹RNG，都看不到这些在幕后默默牺牲的队伍 VIT：说出来你们可能不信，我双杀S7总冠军，打下目前全世界公认final boss RNG，然后我止步16强 安掌门：说出来你们不信，作为这届LOL宣传片的主角，我特么打了一局就回家了，一脸懵逼 古人早就看明白了，知道尺帝没水银就是移动的三百块。 因此道出: 尺帝无银三百两 天若有情天亦老 VIT 我是真的心疼这支来自EU赛区的VIT。他们像极了S6的ANX ———— ANX辅助一手火男输出仅比己方冰鸟低了一丝，打得其他战队头皮发麻，最后与那年被寄予厚望的ROX tiger鏖战66分钟，最终小组第一出线。VIT 今年也是敢打，下路双人组锤石德莱文砍炸RNG，砍翻三星。可惜了，VIT与C9一战，谁输了都是岌岌可危，GEN最后没赢了RNG，也就断绝了VIT。死亡之组B组，真的值得3队出线。对比一下，A组都什么玩意。 锅老师在又是在生死局才上场。莽王之王用奥拉夫在一级拿下一血，让男剑魔吊打慎，然后下路打了两拨高光团战，一路碾压。RNG到现在拿了今年所有冠军，这其中，几乎每次都是锅老师临危救主：春决，MSI，夏季赛决赛，S赛小组出线关键局，出线争第一加赛局。一直不上mlxg让我看不懂，但是一上就是完美团战结束，刺激啊。 A组1234AFS 4-2G2 4-3FW 3-4PVB 2-4 整组都是菜鸡，相互还啄得乐此不疲。VIT看了A组想哭，心疼VIT。 AFS：我S8第一局输了 Gen.G：俺也一样 AFS：我第二局也输了 Gen.G：俺也一样 AFS：我第三局赢了 Gen.G：俺也一样 AFS：…… AFS：我第二轮3-0 Gen.G：样一也俺 AFS：我小组第一出线了 GEN.G：样一也俺 C组1234KT 5-1EDG 4-2TL 3-3MAD 0-6 没啥说的了，虽然我挺RNG冠军，但EDG加油好吧 段友也深情 说实话，edg压力很大，如果他干不过kt第二出现，那么和rng或者ig打内战的可能性是66.6%，如果想赢进4强，那么就必须干掉rng，ig一支队伍，4强后赢不了外队，那会被喷死。假如遇到内战不进4强，是被rng或者ig淘汰，那又是年年8强，也是被喷死。所以edg的出路只剩：小组赛最大努力干掉kt，第一出现避免内战，这是最得到粉丝认可支持不喷的出路。第二出路就是看上帝，抽签遇到a组第一，避开rng和ig，避开内战，这种可能性仅33.3%。第三出路就是内战后干掉ig进4强再进总决赛，可能性也不大。不过干掉rng也会被喷，哪怕得冠，因为rng是夺冠全华班，lpl太需要自己全华班去证明自己赛区了。所以，最好最好的答案就是小组赛把kt压到第二，edg命不好，无奈必须把小组赛当作总决赛对待，因为后面的路不好走，不管是内战还是上帝的关照，都会被喷。还不如小组打出荣誉出来，拼死一搏。 ————引自bilibili 我叫iboy，uzi的i 人间正道是沧桑 “天若有情天亦老，人间正道是沧桑”这两句诗来形容A组和C组再合适不过了，前一句心疼VIT好悲情，后一句劝导EDG别膨胀。 第二轮EDG vs KT，争小组头名关键一战，不被看好的EDG凭借着一手赵信和洛的开团，完美取胜。团前局势为：EDG是蓝色方，占好大龙视野，四人在中路草丛蹲伏，仅上单在中路二塔出防守。关于这次团战，我也有自己的看法：KT知道EDG在大龙蹲伏，所以并不过去，反而是5人抱团准备推中，逼你回来守中————围魏救赵的方式保大龙。关键是：EDG想到了你KT要以攻代守，知道你要推中。那好，待你推过河道，我赵信从草丛闪现e开团直戳后排，你怕不怕。LPL打架是猛，不过这一波，我认为是智商碾压了. 战胜KT，已于KT同分，可以与KT打加赛争头名了。可惜的是，EDG却在TL这小阴沟里翻了船。只能说，这盘EDG膨胀了啊 D组1234FNC 6-1IG 5-2100T 2-4GREX 0-6 D组也没啥说的，本以为IG会轻松6-0第一出线，没想到FNC发挥出色，17日最后两盘比赛，连续战胜IG，IG第二出线。这就难受了，8强碰到RNG就难受了啊。 IG 高光时刻 rookie 妖姬对线阿卡丽，对面盲僧来gank，阿卡丽上来勾引了一波，却瞬间被妖姬打至将死，最后瞎子出q瞬间二段r回去，收掉阿卡丽人头。瞎子悻悻离去。 中路团战IG已经失利，duke 刀妹孤身深入人群，e q r q 闪现 a a q e a超远距离追死残血ez,并收了盲僧人头，然后潇洒离开。挽大厦于将倾。虽然最后还是输了，rookie duke这两拨操作天秀了。 段友·老梗 一代版本一代神，代代中单秀岳伦 年轻就要踏实 怎么说，jkl是真滴太菜，太年强，太慌张，脑子容易短路。瞎子第一波gank下路，jkl不闪现躲第一段q，等二段q上来了才闪现拉开距离，瞎子跟上闪现红buff黏上，jkl难逃送出一血的命。 ning王是也是真滴有问题。ning和jkl感觉就是IG的主要短板，一手瞎子一手酒桶毫无作用。 淘汰赛8强抽签！除了RNG的团战外，最刺激的就是抽签和bp了，实在太心跳了。RNG上上签没抽到EDG或者IG，正巧抽到口嗨的G2。淘汰赛完美剧本，就等明天（10.20）上演： IG首战血拼KT到第五把，靠EDG.DEFT抬一手进入四强。 阿P终于为自己的口嗨付出代价，0:3惨遭淘汰，泪洒首尔。 C9小组赛底牌出尽，拼尽全力仍不敌非洲队，遗憾出局。 EDG一雪前耻，双C发功RAY名带马MEIKO不迷，战胜FNC。 IG RNG重演夏决，皇族挥泪斩IG，ROOKIE小狗历史性拥抱。 EDG7酱上场，打满五局挺进决赛，玄学发功AFS突然暴毙。 在2018的金秋，狗年与猪年交际的十月，在韩国下一场金色的雨。 段友的迭代 RNG要是拿了冠军，我从点赞的人里抽十个人送十套冠军皮肤。哪怕只有一赞，就只送一套。 RNG要是拿了冠军，我从点赞的人里抽十个人送十套上海汤臣一品。哪怕只有一赞，就只送一套。 如果KT夺冠，我从点赞的人里抽十个送十套冠军皮肤，如果RNG夺冠，我一个皮肤也不送，因为RNG真的会夺冠]]></content>
      <categories>
        <category>LOL</category>
      </categories>
      <tags>
        <tag>LOL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Customize elastic search analyzer]]></title>
    <url>%2F2018%2F10%2F12%2Fcustomize-elastic-search-analyzer%2F</url>
    <content type="text"><![CDATA[先放链接，省的找得麻烦 https://legacy.gitbook.com/book/looly/elasticsearch-the-definitive-guide-cn/detailsnote：这个pdf针对1.4版本的es看es，一个链接就够了。在线看非常慢，建议下载pdf 🐷🐷🐷 与本文重点相关的是pdf中的分析、映射两小节。先放总结理解： nosql也都不过如此，hbase mongo es，对比relational db，差别不过是列是可变的 每行的列虽是变化的，但es可以在_mapping中对所有列进行各自的设置 列虽然可变不受限制，但设计时仍然应该将尽可能相同的列放在同一个表下 需求工作中，需要对relational db中表的注释做个搜索功能，便于他人使用。比如：表 my_favorite_fruit ： 需求方小红，负责人小明，comment: 我最爱吃的水果 字段 注释 fruit 水果名称 color 水果颜色 es全文搜索前，会先analyze(分词)然后创建倒排索引。默认的standard analyzer及其他自带的analyzer不会把 my_favorite_fruit 分开，即我搜 favorite 是搜不到的。 我希望全文搜索时，搜索 favorite 可以搜到，搜 水果 可以搜到，但是搜 水 搜不到，并且希望中文的分词更友好（ik ik_max_word）。因此要自定义analyzer。es 的mapping相当于表的metadata，可以对所有的filed（列）配置各自的analyzer 实现 首先建index，同时新建一个自定义analyzer。自定义analyzer的细节见pdf及官网 123456789101112131415161718PUT /awesome_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;pattern&quot;, &quot;pattern&quot;: &quot;_&quot; &#125; &#125; &#125; &#125;&#125; pattern analyzer 的 pattern 遵循java正则语法 例如”pattern”: “_| “ 即按下划线或空格分词 安装ik，细节见ik官网。要注意的就是 1) ik的版本要与es的版本完全一致. 2) ik分词器的选用，我用ik_max_word 设置mapping：表名用my_analyzer，中文字段用ik_max_word 1234567891011121314PUT /awesome_index/_mapping/awesome_table&#123; &quot;properties&quot;: &#123; &quot;table_name&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125;, &quot;comment&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125; 注意：设置setting和设置_mapping，要在PUT入数据之前 PUT 入数据 12345678910111213141516171819202122PUT /awesome_index/awesome_table/1&#123; &quot;table_name&quot;: &quot;my_favorite_fruit&quot;, &quot;person_in_charge&quot;: &quot;小明&quot;, &quot;comment&quot;: &quot;我最爱吃的水果&quot;, &quot;demand&quot;: &quot;小红&quot;, &quot;detail&quot;: [ &#123; &quot;comment&quot;: &quot;名称&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;fruit&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;, &#123; &quot;comment&quot;: &quot;颜色&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;color&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;]&#125; 不足这样就满足需求了。但是，有个问题：detail里的comment没法设置analyzer，还是默认analyzer所以detail中的comment会造成干扰，比如再put一条数据：12345678910111213141516171819202122PUT /awesome_index/awesome_table/2&#123; &quot;table_name&quot;: &quot;my_favorite_vegetables&quot;, &quot;person_in_charge&quot;: &quot;小明&quot;, &quot;comment&quot;: &quot;我最爱吃的蔬菜&quot;, &quot;demand&quot;: &quot;小红&quot;, &quot;detail&quot;: [ &#123; &quot;comment&quot;: &quot;蔬菜名称&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;vegetable&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;, &#123; &quot;comment&quot;: &quot;蔬菜颜色&quot;, &quot;partition&quot;: &quot;null&quot;, &quot;col_name&quot;: &quot;color&quot;, &quot;example&quot;: &quot;&quot;, &quot;data_type&quot;: &quot;string&quot; &#125;]&#125; 搜索 水，是搜不到 _id=1那条的。但搜索 蔬 ，可以搜到 _id=2这条的。如何解决es对 fields 里array的analyzer呢？ 再改进把mapping里detail也设置具体就好了，顿时感觉es真滴nb123456789101112131415161718192021222324252627282930313233343536PUT /awesome_index/_mapping/awesome_table&#123; &quot;properties&quot;: &#123; &quot;table_name&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125;, &quot;comment&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;detail&quot;:&#123; &quot;properties&quot;: &#123; &quot;col_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;:&quot;my_analyzer&quot; &#125;, &quot;data_type&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;comment&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;example&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;partition&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; further more：nested object对于以上一个field里是一个array的情况，可以看 es 的 nested object。简单来说即在设置mapping时：123&quot;detail&quot;:&#123; &quot;type&quot;: &quot;nested&quot; &#125; pdf中，看嵌套章节，或自行百度，不赘述了，最好的当然还是官方文档]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十里飘香 桂树成林]]></title>
    <url>%2F2018%2F10%2F09%2Fguilin%2F</url>
    <content type="text"><![CDATA[国庆全家出游桂林]]></content>
      <categories>
        <category>Travel</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Airflow celery executor]]></title>
    <url>%2F2018%2F09%2F20%2Fairflow-celery-executor%2F</url>
    <content type="text"><![CDATA[airflow celery executor 搭建 airflow是一个任务流工具，它基本的local executor已经可以满足大部分的需求，但是不能在web端点击run。因此配置celery executor来提高可用性。 celerycelery是一个Python写的分布式框架 brokerbroker就是一个消息队列来分发消息，celery推荐Rabbitmq，Redis。个人用rabbitmq1234rabbitmqctl add_user rootrabbitmqctl set_user_tags root administratorrabbitmqctl add_vhost airflow rabbitmqctl set_permissions -p airflow root “.” “.” “.*” 直接设为administrator 2333 workerworker就是执行者，可以是分布式的。在一台机器上的话，worker数量不是越多越好，celery建议worker数量不超过总逻辑cpu数的两倍。 backendbackend用来存储worker执行的结果。我个人用postgresql。当然也可以用Redis或mq 在postgre中建一个专用的库，例如取名：celery_backend_for_airflow 避免权限问题，这个pg库的owner直接就是一个superuser，例如bowen 2333 airflow celery配置在airflow.cfg中的配置就很方便 配broker，就改下 broker_url，例如 1broker_url = amqp://root:111111@localhost:5672/airflow 配backend，就改下 celery_result_backend，格式 1celery_result_backend = db+postgresql://bowen:111111@localhost/celery_backend_for_airflow 然后启动worker celery默认不能以root用户启动worker，启动airflow worker就有了问题： airflow的worker实际就是celery的worker，root下直接 # airflow worker 会报错。 也不能以普通用户运行worker airflow worker 要写入airflow的log，但是普通用户运行的worker没有权限写入以root生成的log文件。 因为log总要生成新的，所以不能通过手动改所以log文件的权限来解决。 所以就有两个解决方法： 1) 修改airflow生成log部分的代码，使之o+w。但是说不定改了这个权限后还有别的权限问题。 2) 修改airflow调用celery的部分，使之可以root运行 参照其他博文，用2，在airflow包里的xxxx/site-packages/airflow/executors里的celery_executor.py加上 12from celery import platformsplatforms.C_FORCE_ROOT = True 之后一顿重启，基本就ok了。有问题可以直接在rabbitmq或者pg中看是哪步出了问题。启动worker可以指定pid file，并且Daemon1airflow worker --pid ~/airflow/airflow-worker.pid -D]]></content>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Hexo]]></title>
    <url>%2F2018%2F09%2F20%2Fabout-hexo%2F</url>
    <content type="text"><![CDATA[hexo-backup记录一下搭建过程中的要注意的点，以及hexo配置，主要是./下和./themes/next/下的两个_config.yml文件。这两个文件在以下就以./和./themes/next/来区分。hexo 是基于nodejs的，我的node: v10.10.0, npm: 6.4.1 安装使用见hexo官网：[https://hexo.io/zh-cn/] deploy 配置 deploy 注意按官网介绍，需要安装插件 1$ npm install hexo-deployer-git --save 托管到 GitHub GitHub 可以建一个特殊的repo来托管网站：repo名字按规则来建 username.github.io。把网站文件push到这个repo下，然后访问https://b0o0wen.github.io/ 在本地在 ./_config.yml最后添加： 1234deploy: type: git repo: git@github.com:b0o0wen/b0o0wen.github.io.git branch: master 注意冒号后一定要有个空格 appearance 配置 主题配置主题使用next: https://github.com/iissnan/hexo-theme-next 我的next版本是 v5.1.2 1$ git clone --branch v5.1.2 https://github.com/iissnan/hexo-theme-next themes/next clone next之后，在./themes/next/下有了另一个_config.yml文件主要有： menu：如tags, categories等。在menu下新建一行xxxx，hexo就会在网站上生成一个名为xxxx的tab，对于常用的menu，hexo还会自带图标。需要注意的是要： 1hexo new page &apos;xxxx&apos; 来在./source/xxxx建立一个index.md，即点击该tab对应的页面。对于tags，在该index.md中修改这个页面的type为tags，hexo就给你生成一个tags页面，还自带词云效果 scheme：hexo共四个scheme avatar：头像的默认根目录在 ./themes/next/source/ 下，支持各种图片格式 social：hexo自带图标 背景图及透明度在./themes/next/source/css/ 下有个_custom文件夹，这里可以修改custom.styl来定制css 12345678910111213141516body &#123;// background:url(https://source.unsplash.com/random/1600x900); background:url(/images/background_leaf.jpg) //背景图片，默认根目录在 ./themes/next/source/ 下 background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%;&#125;//页面上menu那部分class是 .header-inner.header-inner &#123; opacity: 0.88&#125;.main-inner &#123; opacity: 0.9;&#125; 使用技巧摘要写了一篇新post后，web上会默认显示全文，为了只显示部分摘要 可以在文章的md中添加 1&lt;!--more--&gt; 在任意位置截取摘要。 可以修改配置 123auto_excerpt: enable: true length: 150 来截取固定摘要。推荐第一种。 图片 使用hexo的资源文件夹，修改./_config.yml: 1post_asset_folder: true 然后new ‘xxxx’时会有一个同名文件夹 ‘xxxx’，图片example.jpg放其下。 md中用图片时有两种方式： md的语法 1![This is an example image](example.jpg) 方括号中是图片说明，图片名example.jpg前不需要加任何路径，自动相对到文件夹’xxxx’下。这样，图片可以在文章中显示，但是没法在首页显示。因为在首页的路径下，找不到这个图片 hexo的私有语法：“标签” 1&#123;% asset_img example.jpg This is an example image %&#125; asset_img声明是图片资源，同样不用加路径。这样不截断的话，可以在文章和首页同时显示图片 表格 表格无非就是 | - ： 但要注意的是：表格要与前边的内容间隔一行，不然渲染不成功 在添加或修改了一篇post后，hexo g -d 之前，最好 1hexo clean 这会清空public文件夹。如果不clean，而你又恰好修改或删除了一个tag或者category，那么例如在tag页面可能出现：“3 tags in tatol”，但是下边具体的tag只有2个]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F09%2F20%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
