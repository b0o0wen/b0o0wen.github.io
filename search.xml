<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Bitonic Traveling Salesman Problem]]></title>
    <url>%2F2019%2F01%2F02%2Fbitonic-traveling-salesman-problem%2F</url>
    <content type="text"><![CDATA[bitonic旅行商问题是算法导论中的一个例题，也是很经典的一个dp问题。 v-1 dp itdp 的关键就是找到 recursive substructure https://scicomp.stackexchange.com/questions/812/how-to-implement-a-dynamic-programming-solution-to-the-2d-bitonic-euclidean-trav 翻译一下，就是最右点n必须与次右点n-1相连，才能保证bitonic。把n点出发，n-1点结束的最短路径称为bitonic_path而当点 n 与 中间任意点 k 相连时，k+1点到n-1点间所有点必须依次递增相连才能保证bitonic此时可以发现，k点和k+1点成为了相同子结构的bitonic_path问题 对应到Python中，对于长度为n的点list，最右点为n-1方便起见，输入的点list是从左到右的，如果不是的话，加一个排序也很简单 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import numpy as npdef eu_dist(a,b): return np.sqrt(np.sum(np.square(np.array(a)-np.array(b))))class BitonicTSP: def __init__(self, points): self.n = len(points) # 任意两点间线段距离 self.p2p_dist = np.zeros((self.n, self.n)) for i in range(self.n-1): for j in range(i + 1, self.n, 1): self.p2p_dist[i][j] = eu_dist(points[i], points[j]) # 任意两点间，递增线段累积路径 # 如 acc_dist[1][4] = p2p_dist[1][2] + p2p_dist[2][3] + p2p_dist[3][4] self.acc_dist = np.zeros((self.n, self.n)) for i in range(self.n-1): self.acc_dist[i][i+1] = self.p2p_dist[i][i+1] for j in range(i+2,self.n): self.acc_dist[i][j] = self.acc_dist[i][j-1] + self.p2p_dist[j-1][j] self.bitonic_result = &#123;&#125; self.bitonic_result[1] = self.p2p_dist[0][1] # self.bitonic_result[2] = self.p2p_dist[0][1] + self.p2p_dist[0][2] # bitonic path between points[n-2] and points[n-1] def bitonic_path(self,p): if p in self.bitonic_result.keys(): return self.bitonic_result[p] self.bitonic_result[p] = 1000000 # points in [0,p-1], length p # k in [1,n-2] for k in range(0,p-1): self.bitonic_result[p] = min(self.bitonic_result[p], self.p2p_dist[k][p] + self.acc_dist[k+1][p-1] + self.bitonic_path(k+1)) def full_path(self): for i in range(1,self.n): self.bitonic_path(i) print(self.bitonic_result[self.n-1] + self.p2p_dist[self.n-2][self.n-1])if __name__ == '__main__': points = [ [0, 6], [1, 0], [2, 3], [5, 4], [6, 1], [7, 5], [8, 2]] app = BitonicTSP(points) app.full_path() output:125.58402459469133 结果正确 ✔️ 😋 v-2 plot it给定一堆点，我不仅想知道最短距离是多少，更想知道最短距离是如何走的。想一下，v-1 版本的方法似乎是不行的。 v-1是自底向上dp的，而实际在最终的结果中，后边的点对左边的点，是有约束的。那这样自底向上自然是不够的。 [details will be at the end of this part] 并且methodbitonic_path只有一个入参，那就已经假定了bitonic_path 是k点和k+1点间的bitonic_path。这更是一个死穴。严重不足，有待改进。由此有了v-2版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import matplotlib.pyplot as pltimport numpy as npdef eu_dist(a,b): return np.sqrt(np.sum(np.square(np.array(a)-np.array(b))))class BitonicTSP: def __init__(self, points): self.n = len(points) # 任意两点间线段距离 self.p2p_dist = np.zeros((self.n, self.n)) for i in range(self.n-1): for j in range(i + 1, self.n, 1): self.p2p_dist[i][j] = eu_dist(points[i], points[j]) # 任意两点间，递增线段累积路径 # 如 acc_dist[1][4] = p2p_dist[1][2] + p2p_dist[2][3] + p2p_dist[3][4] self.acc_dist = np.zeros((self.n, self.n)) for i in range(self.n-1): self.acc_dist[i][i+1] = self.p2p_dist[i][i+1] for j in range(i+2,self.n): self.acc_dist[i][j] = self.acc_dist[i][j-1] + self.p2p_dist[j-1][j] # bitonic_path 记录以 x,y两点为终点，走过所以小于等于y的点的bitonic_path最小值，不包括 p2p_dist[x][y] # assume x &lt; y # bitonic_path 从底向上刷 self.bitonic_path = np.zeros((self.n, self.n)) self.bitonic_path[0][1] = self.p2p_dist[0][1] # mark_table 记录x,y（同样x&lt;y）两点取得最优bitonic_path时，y点连接的点 # 如若mark_table[5][6] = 3, 则表明，bitonic_path[5][6]最优时，6点连接3点 # mark_table 则需要自顶向下刷 self.mark_table = np.zeros((self.n,self.n)) self.link_dist = &#123;&#125; self.total_journey = 1000000000 # assume x&lt;y # 自底向上 def bitonic_p(self,x,y): if self.bitonic_path[x][y]&gt;0: return self.bitonic_path[x][y] self.bitonic_path[x][y] = 1000000000 if y-x == 1: for k in range(y-1): # self.bitonic_path[x][y] = min(self.bitonic_path[x][y], self.bitonic_path[k][x] + self.p2p_dist[k][y]) tmp = self.bitonic_path[k][x] + self.p2p_dist[k][y] if tmp &lt; self.bitonic_path[x][y]: self.bitonic_path[x][y] = tmp # self.link_dist[y] = k self.mark_table[x][y] = k elif y-x &gt;1: self.bitonic_path[x][y] = self.bitonic_path[x][y-1] + self.p2p_dist[y-1][y] else: print('invalid x, y ') # assume x&lt;y # refresh_mark_table 要自顶向下刷 def refresh_mark_table(self,x,y): if x==0 and y==1: self.mark_table[x][y] = 0 return if y - x ==1: kk = None for k in range(y-1): tmp = self.bitonic_path[k][x] + self.p2p_dist[k][y] # 这里用 &lt;= 而不是 ==。保险起见，避免小数位数等误差导致的错误 if tmp &lt;= self.bitonic_path[x][y]: kk = k break # break 以减少循环次数，但隐隐感觉有风险，万一碰到 两个很相近的距离，而由于小数位数导致的大小比较错误...也许这个问题不存在，不是这篇blog的重点 self.mark_table[x][y] = kk self.refresh_mark_table(kk,x) elif y - x &gt; 1 : self.mark_table[x][y] = y - 1 self.refresh_mark_table(x, y-1) def full_path_with_points_order(self): # 自底向上 刷 bitonic_path for i in range(self.n-1): for j in range(i+1,self.n,1): self.bitonic_p(i,j) # 算出最短距离 for i in range(self.n-1): self.total_journey = min(self.total_journey, self.bitonic_path[i][self.n-1] + self.p2p_dist[i][self.n-1]) # print(self.total_journey) # 自顶向下 刷 mark_table self.refresh_mark_table(self.n-2,self.n-1) for i in range(1,self.n,1): self.link_dist[i] = int(self.mark_table[:,i].max()) f1 = self.n - 1 l1 = [f1] while f1 in self.link_dist.keys(): l1.append(self.link_dist[f1]) f1 = self.link_dist[f1] f2 = self.n - 2 l2 = [f2] while f2 in self.link_dist.keys(): l2.append(self.link_dist[f2]) f2 = self.link_dist[f2] # return 最短路程 与 路径闭环数组 return self.total_journey, l1+l2[:-1][::-1]+[l1[0]]if __name__ == '__main__': points = [ [0, 6], [1, 0], [2, 3], [5, 4], [6, 1], [7, 5], [8, 2]] app = BitonicTSP(points) # app.full_path() (total_journey,path_list) = app.full_path_with_points_order() fig, plot1 = plt.subplots(1, 1) plot1.set_xlabel('x label') plot1.set_ylabel('y label') xs = [points[i][0] for i in path_list] ys = [points[i][1] for i in path_list] plot1.plot(xs, ys, '--o') plt.show() output:12total_journey : 25.58402459469133path_list : [6, 4, 1, 0, 2, 3, 5, 6] refresh_mark_table 这个方法虽然迫不得已又自顶向下循环了一遍，但是其实时间复杂度也是 O(n^2) 并且真实情况大多是远小于这个时间的，而且是用的bitonic_path已经计算好的值，所以影响也不是很大。嘿嘿，还不错。 现在我们回过头来看一下 v-2部分开头说到的问题，我们只取前四个点，运行看看结果1234points = [[0, 6], [1, 0], [2, 3], [5, 4]] output:12total_journey : 17.79248265776948path_list : [3, 0, 1, 2, 3] 去掉点2，3之间的连线，是不是仍然与 pic-2 中 0，1，2，3 四点的连线不一样？（是） v-bug其实早在我最开始思考良久未果时，就先百度了的…百度上很多答案都是相同的思路： http://www.mamicode.com/info-detail-2436181.htmlhttps://blog.csdn.net/u013491262/article/details/22873043 我当时按这个思路写了一份Python版：123456789101112131415161718192021222324252627282930313233343536373839404142# Bitonic 即严格地从最左点走到最右点（去程），然后从最右点返回最左点（返程）。去程中不能有从右往左走的路线，同样返程中也不能有从左往右走的路线。import matplotlib.pyplot as pltimport numpy as nppoints = [[0, 6],[1, 0],[2, 3],[5, 4],[6, 1],[7, 5],[8, 2]]# 输入为以上points(x坐标是递增的)def btsp(points): n = len(points) # 任意两点间距离 dist_table = np.zeros((n, n)) for i in range(n-1): for j in range(i + 1, n, 1): dist_table[i][j] = eu_dist(points[i], points[j]) # result table r = np.zeros((n,n)) r[:, :] = 'inf' # 令 i &lt; j r[1,2] = dist_table[0,1] + dist_table[0,2] # j in [2,n-2] n-1是最后终点了 for j in range(2,n-1): for i in range(1,j): # print(i,' , ',j,' - ',r[i,j]) # print(i,' ', j+1,' ',r[i,j+1]) # print(j,' ',j+1,' ',r[j,j+1]) r[i,j+1] = min(r[i,j]+dist_table[j,j+1],r[i,j+1]) r[j,j+1] = min(r[i,j]+dist_table[i,j+1],r[j,j+1]) ans = 1000000000 # i in [1,n-2] for i in range(1,n-1): ans = min(ans,r[i,n-1]+dist_table[i,n-1]) return ansdef eu_dist(a,b): return np.sqrt(np.sum(np.square(np.array(a)-np.array(b))))ans = btsp(points)print(ans) 这个v-bug版本不仅难以理解，而且，有bug：我们还是只取前4个点，看下跑出来的结果output:118.507445715422968 这个结果，大于我们 pic-3 中计算的结果。其实它这个18.5是以下图中红色路线行驶的 两种方法都是以2，3 点做bitonic_path。图中： 灰线连接点2，3，是公共的 蓝线是v-2的方法 红线是v-bug的方法。 虽然对于例题，v-bug得到的结果是正确的，但是当4个点时，它显然是错误的。不过也算有点 thought-provoking 吧。 第一个链接里还有这样一句话： 思考一下, 是不是每一条走完{1..n}的路线都存在一个走完{1..i}(i&lt;n)的子路线? 所以不会漏. 你的良心真的不会痛么。 这个故事告诉我们： 还是要自己想想，盲信他人不靠谱 想不出来，就还是Google吧，百度不靠谱…]]></content>
      <tags>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib]]></title>
    <url>%2F2018%2F12%2F21%2Fmatplotlib%2F</url>
    <content type="text"><![CDATA[matplotlib 很综合，所以它的api也有些复杂。以 matplotlib.pyplot 为例，它主要有两个level的api： pyplot 的两个api level这两个 level api 功能大致一样，可以互相转换。对于一般的需求，官方document 的意思是推荐 object level pyplot level At this level, simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure 123456import matplotlib.pyplot as pltplt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--o')plt.xlabel('x axis')plt.ylabel('y axis')plt.axis([0, 6, 0, 20])plt.show() object level At this level, the user uses pyplot to create figures, and through those figures, one or more axes objects can be created. These axes objects are then used for most plotting actions. 12345678import matplotlib.pyplot as pltfig, a = plt.subplots()a.set_xlabel('x axis')a.set_ylabel('y axis')a.set_xlim(0,6)a.set_ylim(0,20)a.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--o')plt.show() Again, for these simple examples this style seems like overkill, however once the graphs get slightly more complex it pays off. detail doc (object lvl) ax.set_xlim(0,6) 与 ax.set_xbound(0,6) 的区别 https://stackoverflow.com/questions/11459672/in-matplotlib-what-is-the-difference-betweent-set-xlim-and-set-xbound 先不练习了，这个已经够画旅行者问题了]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux frequantly used command]]></title>
    <url>%2F2018%2F12%2F18%2Flinux-frequantly-used-command%2F</url>
    <content type="text"><![CDATA[lsof, netstat, grep -v grep, awk, xargs, kill -9 查看占用端口的进程1.1lsof -i:5000 输出12COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEpython3.6 1468 root 5u IPv4 66961000 0t0 TCP *:commplex-main (LISTEN) 2.1netstat -apn | grep 5000 最后一项显示的是pid和对应的名称1tcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN 1468/python 批量kill进程1ps -ef | grep test | grep -v grep | awk '&#123;print $2&#125;' | xargs kill -9 awk 行处理器，以空格为默认分隔符将每行切片xargs 命令是给命令传递参数的过滤器，把标准数据转换成命令行参数。在这里则是将获取前一个命令的标准输出然后转换成命令行参数传递给后面的kill命令kill的-9 强制结束]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[no looking down on gitignore]]></title>
    <url>%2F2018%2F12%2F14%2Fno-looking-down-on-gitignore%2F</url>
    <content type="text"><![CDATA[对已在版本库中的文件，.gitignore不起作用1git rm -r --cached . 然后再修改.gitignore，当然还需要再提交一下12git add .git commit -m 'update .gitignore' 彻底删除版本库中涉敏文件一个高级的命令：git filter-branch如果一些敏感文件在最初已经提交过了，然后随着项目的进行，想起来应该避免提交包含数据库帐密的文件。但此时，通过以上修改 .gitignore，然后删除工作区文件，再提交这样是不行的！虽然在最新的版本中，已经删除了，但是在这次历史版本中还是有的。此时就需要使用git filter-branch 例如，以下util路径下的mysql_config.json是我们要在历史版本中删除的涉敏文件1git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch util/mysql_config.json' --prune-empty --tag-name-filter cat -- --all 这样，所有的commit历史纪录都会被改写。 在本地下的commit历史已经被改了，但是远程仓库的还未变。在远程仓库中删除涉敏文件才是我们根本目的啊。而此时git push origin master报错了。 这种情况下就需要1git push origin master --force 我们可以看到--force的释义如下1--force -- allow refs that are not ancestors to be updated 参考自 https://www.jianshu.com/p/17e5adf47da5]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux service and its auto start-up]]></title>
    <url>%2F2018%2F12%2F14%2Flinux-service-and-its-auto-start-up%2F</url>
    <content type="text"><![CDATA[添加到service很简单，就是service会到 /etc/init.d 下寻找可执行脚本，然后运行该脚本。以我新添加的service elasticsearch start 为例：在 /etc/init.d 下添加 elasticsearch 脚本，权限 755脚本内容如下： 12345678910111213141516171819202122232425262728#!/bin/bash#chkconfig:2345 80 05 --指定在哪几个级别执行。80为启动的优先级，05为关闭的优先级。启动与关闭都是，数字小的优先执行#description: elasticsearch start-upRETVAL=0start()&#123; su - es -c "cd elasticsearch-6.3.2/bin &amp;&amp; ./elasticsearch -d" echo '[start]'&#125;stop()&#123; espid=`lsof -i:9200|sed '1d'|awk '&#123;print $2&#125;' | sed -n '1p'` kill -9 $espid echo '[stop]'&#125;case $1 in start) start ;; stop) stop ;; restart) stop start ;;esacexit $RETVAL service的添加是即时生效的，不需任何重启，这样就可以了 开机启动chkconfig是用于把服务加到开机自动启动列表里12chkconfig --add elasticsearch chkconfig elasticsearch on 或 off 然后在 etc/rc.d/rc2 3 4 5.d 下就会看到 S80elasticsearch，添加到自启动成功S80即其启动的优先级，关闭是K05 [kill]。启动与关闭都是，数字小的优先执行。 附：Linux 的7个运行级别运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 所以一般都是2345… 脚本中的几个Linux命令su - es -c “xxxx” su es，并且采用es用户的环境变量，来执行后边的命令 “xxxx” 这条命令对自动执行各种不能以root执行的程序，实在是太实用了 awk awk 是行处理工具，是一行一行地对行内的内容处理，例如修改每行中的一些字符 awk对一行处理时，对行内元素，默认以空格为分割符，也可以自定义分割符 sed sed 也是行处理工具，它是一次处理一行，例如删除某行 sed 的参数与vim命令类似，例如 ‘1d’ 删除第一行]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es systematic design]]></title>
    <url>%2F2018%2F12%2F01%2Fes-systematic-design%2F</url>
    <content type="text"><![CDATA[back up a es design1234567891011121314151617181920212223242526272829303132PUT /dw_table&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "my_analyzer": &#123; "tokenizer": "my_tokenizer", "filter":["lowercase","my_stopwords"] &#125;, "my_not_analyzed_analyzer": &#123; "tokenizer": "my_not_analyzed_tokenizer" &#125; &#125;, "tokenizer": &#123; "my_tokenizer": &#123; "type": "pattern", "pattern": "_| " &#125;, "my_not_analyzed_tokenizer":&#123; "type": "pattern", "pattern": "this_is_a_pattern_that_your_query_str_cannot_contain" &#125; &#125;, "filter": &#123; "my_stopwords":&#123; "type":"stop", "stopwords":["base","report"] &#125; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354PUT /dw_table/_mapping/awesome_table&#123; "properties": &#123; "table_name":&#123; "type": "text", "analyzer": "my_analyzer" &#125;, "describe": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125;, "charge":&#123; "type": "text", "analyzer": "my_not_analyzed_analyzer", "search_analyzer": "my_analyzer" &#125;, "demand":&#123; "type": "text", "analyzer": "my_not_analyzed_analyzer", "search_analyzer": "my_analyzer" &#125;, "detail": &#123; "properties": &#123; "col_name": &#123; "type": "text", "analyzer":"my_analyzer" &#125;, "data_type": &#123; "type": "keyword", "index": false &#125;, "describe": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125;, "example": &#123; "type": "text", "analyzer": "standard" &#125;, "partition": &#123; "type": "keyword", "index": false &#125; &#125; &#125;, "detail_describe": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125; &#125;&#125; 象征性地插入一条数据123456789101112131415161718192021222324252627282930313233343536373839PUT /dw_table/awesome_table/1&#123; "table_name": "base_m_searchid_to_keyword", "detail": [ &#123; "describe": "分区字段", "partition": "partition_col", "col_name": "date", "example": "", "data_type": "string" &#125;, &#123; "describe": "是否包含敏感词", "partition": "null", "col_name": "is_spam", "example": "", "data_type": "int" &#125;, &#123; "describe": "搜索词", "partition": "null", "col_name": "keyword", "example": "", "data_type": "string" &#125;, &#123; "describe": "一次搜索的唯一编号", "partition": "null", "col_name": "searchid", "example": "", "data_type": "string" &#125; ], "charge": "cindy", "describe": "searchid对应的关键词相关信息", "demand": "cindy", "detail_describe": "searchid和搜索词的关联表", "v": 1&#125; 只有es分析器和mapping是不够的，还要有后端针对不同的输入字符串而定制的查询方式。查询设计一：例如以下query_str 是完整字符串，str_en 就是输入字符中的英文字符，str_ch是中文字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950query = &#123; "from": 0,"size":20, "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "table_name": &#123; "query": str_en, "boost": 5 &#125; &#125; &#125;, &#123; "match": &#123; "describe": &#123; "query": str_ch, "boost": 2 &#125; &#125; &#125;, &#123; "match": &#123; "charge": query_str &#125; &#125;, &#123; "match": &#123; "demand": query_str &#125; &#125;, &#123; "match": &#123; "detail.col_name": str_en &#125; &#125;, &#123; "match": &#123; "detail.describe": str_ch &#125; &#125;, &#123; "match": &#123; "detail.example": query_str &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es -- not_analyzed]]></title>
    <url>%2F2018%2F11%2F08%2Fes-not_analyzed%2F</url>
    <content type="text"><![CDATA[es版本6.4 使某field建立索引时不分词，但是搜索时分词 很多教程中说，在mapping中设置field不分词为 &quot;index&quot;:&quot;not_analyzed&quot;，但是版本更新，这个语法已经错误了，&quot;index&quot;的值只能是true和false，当值为false时，是表示这个field不被索引，而不是不analyze。去查询一个&quot;index&quot;:false的field，会报错400错误如下：1"reason": "Cannot search on field [demand] since it is not indexed." 那该如何改？见以下链接： https://www.elastic.co/blog/strings-are-dead-long-live-strings 就是 &quot;type&quot;: &quot;keyword&quot; 代替了 &quot;not_analyzed&quot; 但有个问题是，把一个field设了&quot;type&quot;: &quot;keyword&quot;后，不能再设置它的 &quot;search_analyzer&quot;，即123456789PUT /dw/_mapping/test&#123; "properties": &#123; "field_1":&#123; "type": "keyword", "search_analyzer":"standard" &#125; &#125;&#125; 这样是会报错的：1"reason": "Mapping definition for [col1] has unsupported parameters: [search_analyzer : standard]" 这就导致了&quot;type&quot;: &quot;keyword&quot;的局限性，具体对比如下 1234567891011121314151617PUT /dw/_mapping/test&#123; "properties": &#123; "field_1":&#123; "type": "keyword" &#125;, "field_2":&#123; "type": "text" &#125; &#125;&#125;PUT /dw/test/1&#123; "field_1":"hello world", "field_2":"hello world"&#125; match / “hello” “hello world” “hello world kidult” “field_1” × ✔️ × “field_2” ✔️ ✔️ ✔️ 我希望表格中右上角那个 × 是 ✔️至此，已经明了了。为了实现需求，我的解决方法是：对于field_1，还是用&quot;type&quot;: &quot;text&quot;，然后analyzer用一个自定义的fake_analyzer——它来实现不分词search_analyzer用另一个analyzer，例如&quot;standard&quot; ps:上边已经说清楚”keyword”了，不过写都写到这儿了，就顺便记一下term，term把整个query当做一个词，运行如下： term / “hello” “hello world” “hello world kidult” “field_1” × ✔️ × “field_2” ✔️ × ×]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming exercise]]></title>
    <url>%2F2018%2F10%2F30%2FDynamic-Programming-exercise%2F</url>
    <content type="text"><![CDATA[Those who cannt remember the past are condemned to repeat it把问题划分为重复性的子问题，并记住计算过的部分来避免重复计算 STEPS Characterize the structure of an optimal solution. Recursively define the value of an optimal solution. Compute the value of an optimal solution. Construct an optimal solution from computed information. 近来闲暇，看看dp做做简单练习。摘要第一句引自博客，STEPS引自算法导论，感觉别人总结的真是到位 先边看导论，边做小练习，做多了再自行归纳 上台阶每次可以上一个或两个台阶，上到n级台阶，问共有多少种上法稍微分析便发现其实是个 斐波那契数列12345678910111213141516171819202122# 自顶向下def upstairs(n): s=&#123;1:1,2:2&#125; if n in s.keys(): return s[n] for i in range(3,n+1): s[i] = s[i-1]+s[i-2] return s[n]# 自底向上s=&#123;1:1,2:2&#125;def upstairs2(n): if n in s.keys(): return s[n] s[n]=upstairs2(n-1)+upstairs2(n-2) return s[n]p=upstairs(10)q=upstairs2(10)print(p,q) 硬币找零假设有三种类型的硬币1，3，5，要找零一定数额n,问最少要多少枚硬币。12345678910111213141516171819202122# 带备忘录自顶向下s = &#123;1:1,2:2,3:1,4:2,5:1&#125;def coins(n): if n in s.keys(): return s[n] s[n] = min(coins(n-1),coins(n-3),coins(n-5))+1 return s[n]# 自底向上def coins2(n): s = &#123;1: 1, 2: 2, 3: 1, 4: 2, 5: 1&#125; if n in s.keys(): return s[n] for i in range(6,n+1): s[i] = min(s[i - 1], s[i - 3], s[i - 5]) + 1 return s[n]x=coins(154)y=coins2(154)print(x,y) 切钢条算法导论书中原例：不同长度的钢条对应着不同的售价，对于长度为n的钢条，问最大售价是多少12345678910111213141516171819202122232425262728293031323334353637383940p=&#123;1:1,2:5,3:8,4:9,5:10,6:17&#125;# 硬核循环def rod_cut1(n): max_price = 0 for i in range(1,n+1): tmp_max = p[i]+rod_cut1(n-i) if tmp_max&gt;max_price: max_price = tmp_max return max_price# 自顶向下r=&#123;&#125;def rod_cut2(n): if n in r.keys(): return r[n] r[n] = 0 for i in range(1,n+1): tmp_max = p[i]+rod_cut2(n-i) if tmp_max&gt;r[n]: r[n] = tmp_max return r[n]# 自底向上def rod_cut3(n): r = &#123;0:0&#125; for i in range(1,n+1): a = 0 for j in range(1,i+1): tmp_max = p[j]+r[i-j] if tmp_max&gt;a: a=tmp_max r[i]=a return r[n]x=rod_cut1(5)y=rod_cut2(6)z=rod_cut3(5)print(x,y,z) 捡苹果一个m x n 的格子阵里，每个格子里有一些苹果。从左上角开始，每次只能向右或向下移动一格，每到一个格子就把格子里的苹果捡起来。问移动到特定格子，最多能捡多少个苹果12345678910111213141516171819202122232425apple=[[1,8,3,2],[4,3,6,7],[4,9,8,7]]# 带备忘录自顶向下import numpy as npr = np.zeros((3,4))def max_apple(x,y): if r[x][y] &gt; 0: return r[x][y] if x != 0 and y != 0: r[x][y]=max(max_apple(x-1,y),max_apple(x,y-1))+apple[x][y] elif x==0 and y!=0: r[x][y]=max_apple(x,y-1)+apple[x][y] elif x!=0 and y==0: r[x][y] = max_apple(x-1, y) + apple[x][y] else: r[x][y]=apple[x][y] return r[x][y]x = max_apple(2,2)print(x)# 自底向上]]></content>
      <tags>
        <tag>algorithm</tag>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LOL S8 Log]]></title>
    <url>%2F2018%2F10%2F15%2FLOL-S8-Log%2F</url>
    <content type="text"><![CDATA[入围赛不提了好吧，可以说LPL三号种子EDG无悬念出线 小组赛B组1234RNG 5-2C9 4-2VIT 3-2GEN 1-5 RNG 高光时刻 组内首轮RNG vs GEN。GEN青钢影带线几近带到高地了，ming洛看似一手失败的大招被霞大招躲了，但细心看就会发现这是RNG的战术————我知道你霞有大招，我洛大招cd比你短，这波就是逼你换大招的，我洛连闪现都没用。然后letme一波完美开车，瞎子洛逼霞走位，霞慌忙交出闪现，也躲不过这场车祸。连高地都带掉了的青钢影懵逼着就回家了。GEN还是不能阻止一波暴毙。RNG抓机会打团是真的无敌，瞬间暴毙看着真的爽。 第二轮 RNG 爆锤 GEN。前几级侧漏霸气的小团都不值得提了，最高光莫过于：在红色方蓝buff处，冰女定住刀妹开团，RNG输出差些许刀妹行至二塔处苟活，此时下路一塔还在。瞬间！UZI滑板鞋给锤石大招，从蛤蟆处闪现过墙，然后锤石砸下去击飞三人，让男剑魔开大天神下凡接连击飞，冰女奥拉夫滑板鞋跟上输出，打出一波小团灭。完美combo把GEN心理防线都摧毁了。 状态来了就是神挡杀神，RNG show time!最后一场争第一的加赛C9，因为阵容原因，几乎所有人都认为时间已经到后期，RNG胜算很低了，甚至有人说：单看阵容，不知道右边怎么输。然而，RNG这个名字，100%的胜利象征。太刺激了. 段友·真人才 三星真兄弟，掀翻SKT，拦住格里芬，拖住KZ，咬了C9一口，临走还送了RNG两分，最后连名字都不留下，你们只知道吹RNG，都看不到这些在幕后默默牺牲的队伍 VIT：说出来你们可能不信，我双杀S7总冠军，打下目前全世界公认final boss RNG，然后我止步16强 安掌门：说出来你们不信，作为这届LOL宣传片的主角，我特么打了一局就回家了，一脸懵逼 古人早就看明白了，知道尺帝没水银就是移动的三百块。 因此道出: 尺帝无银三百两 天若有情天亦老 VIT 我是真的心疼这支来自EU赛区的VIT。他们像极了S6的ANX ———— ANX辅助一手火男输出仅比己方冰鸟低了一丝，打得其他战队头皮发麻，最后与那年被寄予厚望的ROX tiger鏖战66分钟，最终小组第一出线。VIT 今年也是敢打，下路双人组锤石德莱文砍炸RNG，砍翻三星。可惜了，VIT与C9一战，谁输了都是岌岌可危，GEN最后没赢了RNG，也就断绝了VIT。死亡之组B组，真的值得3队出线。对比一下，A组都什么玩意。 锅老师在又是在生死局才上场。莽王之王用奥拉夫在一级拿下一血，让男剑魔吊打慎，然后下路打了两拨高光团战，一路碾压。RNG到现在拿了今年所有冠军，这其中，几乎每次都是锅老师临危救主：春决，MSI，夏季赛决赛，S赛小组出线关键局，出线争第一加赛局。一直不上mlxg让我看不懂，但是一上就是完美团战结束，刺激啊。 A组1234AFS 4-2G2 4-3FW 3-4PVB 2-4 整组都是菜鸡，相互还啄得乐此不疲。VIT看了A组想哭，心疼VIT。 AFS：我S8第一局输了 Gen.G：俺也一样 AFS：我第二局也输了 Gen.G：俺也一样 AFS：我第三局赢了 Gen.G：俺也一样 AFS：…… AFS：我第二轮3-0 Gen.G：样一也俺 AFS：我小组第一出线了 GEN.G：样一也俺 C组1234KT 5-1EDG 4-2TL 3-3MAD 0-6 没啥说的了，虽然我挺RNG冠军，但EDG加油好吧 段友也深情 说实话，edg压力很大，如果他干不过kt第二出现，那么和rng或者ig打内战的可能性是66.6%，如果想赢进4强，那么就必须干掉rng，ig一支队伍，4强后赢不了外队，那会被喷死。假如遇到内战不进4强，是被rng或者ig淘汰，那又是年年8强，也是被喷死。所以edg的出路只剩：小组赛最大努力干掉kt，第一出现避免内战，这是最得到粉丝认可支持不喷的出路。第二出路就是看上帝，抽签遇到a组第一，避开rng和ig，避开内战，这种可能性仅33.3%。第三出路就是内战后干掉ig进4强再进总决赛，可能性也不大。不过干掉rng也会被喷，哪怕得冠，因为rng是夺冠全华班，lpl太需要自己全华班去证明自己赛区了。所以，最好最好的答案就是小组赛把kt压到第二，edg命不好，无奈必须把小组赛当作总决赛对待，因为后面的路不好走，不管是内战还是上帝的关照，都会被喷。还不如小组打出荣誉出来，拼死一搏。 ————引自bilibili 我叫iboy，uzi的i 人间正道是沧桑 “天若有情天亦老，人间正道是沧桑”这两句诗来形容A组和C组再合适不过了，前一句心疼VIT好悲情，后一句劝导EDG别膨胀。 第二轮EDG vs KT，争小组头名关键一战，不被看好的EDG凭借着一手赵信和洛的开团，完美取胜。团前局势为：EDG是蓝色方，占好大龙视野，四人在中路草丛蹲伏，仅上单在中路二塔出防守。关于这次团战，我也有自己的看法：KT知道EDG在大龙蹲伏，所以并不过去，反而是5人抱团准备推中，逼你回来守中————围魏救赵的方式保大龙。关键是：EDG想到了你KT要以攻代守，知道你要推中。那好，待你推过河道，我赵信从草丛闪现e开团直戳后排，你怕不怕。LPL打架是猛，不过这一波，我认为是智商碾压了. 战胜KT，已于KT同分，可以与KT打加赛争头名了。可惜的是，EDG却在TL这小阴沟里翻了船。只能说，这盘EDG膨胀了啊 D组1234FNC 6-1IG 5-2100T 2-4GREX 0-6 D组也没啥说的，本以为IG会轻松6-0第一出线，没想到FNC发挥出色，17日最后两盘比赛，连续战胜IG，IG第二出线。这就难受了，8强碰到RNG就难受了啊。 IG 高光时刻 rookie 妖姬对线阿卡丽，对面盲僧来gank，阿卡丽上来勾引了一波，却瞬间被妖姬打至将死，最后瞎子出q瞬间二段r回去，收掉阿卡丽人头。瞎子悻悻离去。 中路团战IG已经失利，duke 刀妹孤身深入人群，e q r q 闪现 a a q e a超远距离追死残血ez,并收了盲僧人头，然后潇洒离开。挽大厦于将倾。虽然最后还是输了，rookie duke这两拨操作天秀了。 段友·老梗 一代版本一代神，代代中单秀岳伦 年轻就要踏实 怎么说，jkl是真滴太菜，太年强，太慌张，脑子容易短路。瞎子第一波gank下路，jkl不闪现躲第一段q，等二段q上来了才闪现拉开距离，瞎子跟上闪现红buff黏上，jkl难逃送出一血的命。 ning王是也是真滴有问题。ning和jkl感觉就是IG的主要短板，一手瞎子一手酒桶毫无作用。 淘汰赛8强抽签！除了RNG的团战外，最刺激的就是抽签和bp了，实在太心跳了。RNG上上签没抽到EDG或者IG，正巧抽到口嗨的G2。淘汰赛完美剧本，就等明天（10.20）上演： IG首战血拼KT到第五把，靠EDG.DEFT抬一手进入四强。 阿P终于为自己的口嗨付出代价，0:3惨遭淘汰，泪洒首尔。 C9小组赛底牌出尽，拼尽全力仍不敌非洲队，遗憾出局。 EDG一雪前耻，双C发功RAY名带马MEIKO不迷，战胜FNC。 IG RNG重演夏决，皇族挥泪斩IG，ROOKIE小狗历史性拥抱。 EDG7酱上场，打满五局挺进决赛，玄学发功AFS突然暴毙。 在2018的金秋，狗年与猪年交际的十月，在韩国下一场金色的雨。 段友的迭代 RNG要是拿了冠军，我从点赞的人里抽十个人送十套冠军皮肤。哪怕只有一赞，就只送一套。 RNG要是拿了冠军，我从点赞的人里抽十个人送十套上海汤臣一品。哪怕只有一赞，就只送一套。 如果KT夺冠，我从点赞的人里抽十个送十套冠军皮肤，如果RNG夺冠，我一个皮肤也不送，因为RNG真的会夺冠]]></content>
      <categories>
        <category>LOL</category>
      </categories>
      <tags>
        <tag>LOL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Customize elastic search analyzer]]></title>
    <url>%2F2018%2F10%2F12%2Fcustomize-elastic-search-analyzer%2F</url>
    <content type="text"><![CDATA[先放链接，省的找得麻烦 https://legacy.gitbook.com/book/looly/elasticsearch-the-definitive-guide-cn/detailsnote：这个pdf针对1.4版本的es看es，一个链接就够了。在线看非常慢，建议下载pdf 🐷🐷🐷 与本文重点相关的是pdf中的分析、映射两小节。先放总结理解： nosql也都不过如此，hbase mongo es，对比relational db，差别不过是列是可变的 每行的列虽是变化的，但es可以在_mapping中对所有列进行各自的设置 列虽然可变不受限制，但设计时仍然应该将尽可能相同的列放在同一个表下 需求工作中，需要对relational db中表的注释做个搜索功能，便于他人使用。比如：表 my_favorite_fruit ： 需求方小红，负责人小明，comment: 我最爱吃的水果 字段 注释 fruit 水果名称 color 水果颜色 es全文搜索前，会先analyze(分词)然后创建倒排索引。默认的standard analyzer及其他自带的analyzer不会把 my_favorite_fruit 分开，即我搜 favorite 是搜不到的。 我希望全文搜索时，搜索 favorite 可以搜到，搜 水果 可以搜到，但是搜 水 搜不到，并且希望中文的分词更友好（ik ik_max_word）。因此要自定义analyzer。es 的mapping相当于表的metadata，可以对所有的filed（列）配置各自的analyzer 实现 首先建index，同时新建一个自定义analyzer。自定义analyzer的细节见pdf及官网 123456789101112131415161718PUT /awesome_index&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "my_analyzer": &#123; "tokenizer": "my_tokenizer" &#125; &#125;, "tokenizer": &#123; "my_tokenizer": &#123; "type": "pattern", "pattern": "_" &#125; &#125; &#125; &#125;&#125; pattern analyzer 的 pattern 遵循java正则语法 例如”pattern”: “_| “ 即按下划线或空格分词 安装ik，细节见ik官网。要注意的就是 1) ik的版本要与es的版本完全一致. 2) ik分词器的选用，我用ik_max_word 设置mapping：表名用my_analyzer，中文字段用ik_max_word 1234567891011121314PUT /awesome_index/_mapping/awesome_table&#123; "properties": &#123; "table_name":&#123; "type": "text", "analyzer": "my_analyzer" &#125;, "comment": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125; &#125;&#125; 注意：设置setting和设置_mapping，要在PUT入数据之前 PUT 入数据 12345678910111213141516171819202122PUT /awesome_index/awesome_table/1&#123; "table_name": "my_favorite_fruit", "person_in_charge": "小明", "comment": "我最爱吃的水果", "demand": "小红", "detail": [ &#123; "comment": "名称", "partition": "null", "col_name": "fruit", "example": "", "data_type": "string" &#125;, &#123; "comment": "颜色", "partition": "null", "col_name": "color", "example": "", "data_type": "string" &#125;]&#125; 不足这样就满足需求了。但是，有个问题：detail里的comment没法设置analyzer，还是默认analyzer所以detail中的comment会造成干扰，比如再put一条数据：12345678910111213141516171819202122PUT /awesome_index/awesome_table/2&#123; "table_name": "my_favorite_vegetables", "person_in_charge": "小明", "comment": "我最爱吃的蔬菜", "demand": "小红", "detail": [ &#123; "comment": "蔬菜名称", "partition": "null", "col_name": "vegetable", "example": "", "data_type": "string" &#125;, &#123; "comment": "蔬菜颜色", "partition": "null", "col_name": "color", "example": "", "data_type": "string" &#125;]&#125; 搜索 水，是搜不到 _id=1那条的。但搜索 蔬 ，可以搜到 _id=2这条的。如何解决es对 fields 里array的analyzer呢？ 再改进把mapping里detail也设置具体就好了，顿时感觉es真滴nb123456789101112131415161718192021222324252627282930313233343536PUT /awesome_index/_mapping/awesome_table&#123; "properties": &#123; "table_name":&#123; "type": "text", "analyzer": "my_analyzer" &#125;, "comment": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125;, "detail":&#123; "properties": &#123; "col_name": &#123; "type": "text", "analyzer":"my_analyzer" &#125;, "data_type": &#123; "type": "text" &#125;, "comment": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125;, "example": &#123; "type": "text" &#125;, "partition": &#123; "type": "text" &#125; &#125; &#125; &#125;&#125; further more：nested object对于以上一个field里是一个array的情况，可以看 es 的 nested object。简单来说即在设置mapping时：123"detail":&#123; "type": "nested" &#125; pdf中，看嵌套章节，或自行百度，不赘述了，最好的当然还是官方文档]]></content>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十里飘香 桂树成林]]></title>
    <url>%2F2018%2F10%2F09%2Fguilin%2F</url>
    <content type="text"><![CDATA[国庆全家出游桂林]]></content>
      <categories>
        <category>Travel</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Airflow celery executor]]></title>
    <url>%2F2018%2F09%2F20%2Fairflow-celery-executor%2F</url>
    <content type="text"><![CDATA[airflow celery executor 搭建 airflow是一个任务流工具，它基本的local executor已经可以满足大部分的需求，但是不能在web端点击run。因此配置celery executor来提高可用性。 celerycelery是一个Python写的分布式框架 brokerbroker就是一个消息队列来分发消息，celery推荐Rabbitmq，Redis。个人用rabbitmq1234rabbitmqctl add_user rootrabbitmqctl set_user_tags root administratorrabbitmqctl add_vhost airflow rabbitmqctl set_permissions -p airflow root “.” “.” “.*” 直接设为administrator 2333 workerworker就是执行者，可以是分布式的。在一台机器上的话，worker数量不是越多越好，celery建议worker数量不超过总逻辑cpu数的两倍。 backendbackend用来存储worker执行的结果。我个人用postgresql。当然也可以用Redis或mq 在postgre中建一个专用的库，例如取名：celery_backend_for_airflow 避免权限问题，这个pg库的owner直接就是一个superuser，例如bowen 2333 airflow celery配置在airflow.cfg中的配置就很方便 配broker，就改下 broker_url，例如 1broker_url = amqp://root:111111@localhost:5672/airflow 配backend，就改下 celery_result_backend，格式 1celery_result_backend = db+postgresql://bowen:111111@localhost/celery_backend_for_airflow 然后启动worker celery默认不能以root用户启动worker，启动airflow worker就有了问题： airflow的worker实际就是celery的worker，root下直接 # airflow worker 会报错。 也不能以普通用户运行worker airflow worker 要写入airflow的log，但是普通用户运行的worker没有权限写入以root生成的log文件。 因为log总要生成新的，所以不能通过手动改所以log文件的权限来解决。 所以就有两个解决方法： 1) 修改airflow生成log部分的代码，使之o+w。但是说不定改了这个权限后还有别的权限问题。 2) 修改airflow调用celery的部分，使之可以root运行 参照其他博文，用2，在airflow包里的xxxx/site-packages/airflow/executors里的celery_executor.py加上 12from celery import platformsplatforms.C_FORCE_ROOT = True 之后一顿重启，基本就ok了。有问题可以直接在rabbitmq或者pg中看是哪步出了问题。启动worker可以指定pid file，并且Daemon1airflow worker --pid ~/airflow/airflow-worker.pid -D]]></content>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Hexo]]></title>
    <url>%2F2018%2F09%2F20%2Fabout-hexo%2F</url>
    <content type="text"><![CDATA[hexo-backup记录一下搭建过程中的要注意的点，以及hexo配置，主要是./下和./themes/next/下的两个_config.yml文件。这两个文件在以下就以./和./themes/next/来区分。hexo 是基于nodejs的，我的node: v10.10.0, npm: 6.4.1 安装使用见hexo官网：[https://hexo.io/zh-cn/] deploy 配置 deploy 注意按官网介绍，需要安装插件 1$ npm install hexo-deployer-git --save 托管到 GitHub GitHub 可以建一个特殊的repo来托管网站：repo名字按规则来建 username.github.io。把网站文件push到这个repo下，然后访问https://b0o0wen.github.io/ 在本地在 ./_config.yml最后添加： 1234deploy: type: git repo: git@github.com:b0o0wen/b0o0wen.github.io.git branch: master 注意冒号后一定要有个空格 appearance 配置 主题配置主题使用next: https://github.com/iissnan/hexo-theme-next 我的next版本是 v5.1.2 1$ git clone --branch v5.1.2 https://github.com/iissnan/hexo-theme-next themes/next clone next之后，在./themes/next/下有了另一个_config.yml文件主要有： menu：如tags, categories等。在menu下新建一行xxxx，hexo就会在网站上生成一个名为xxxx的tab，对于常用的menu，hexo还会自带图标。需要注意的是要： 1hexo new page 'xxxx' 来在./source/xxxx建立一个index.md，即点击该tab对应的页面。对于tags，在该index.md中修改这个页面的type为tags，hexo就给你生成一个tags页面，还自带词云效果 scheme：hexo共四个scheme avatar：头像的默认根目录在 ./themes/next/source/ 下，支持各种图片格式 social：hexo自带图标 背景图及透明度在./themes/next/source/css/ 下有个_custom文件夹，这里可以修改custom.styl来定制css 12345678910111213141516body &#123;// background:url(https://source.unsplash.com/random/1600x900); background:url(/images/background_leaf.jpg) //背景图片，默认根目录在 ./themes/next/source/ 下 background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%;&#125;//页面上menu那部分class是 .header-inner.header-inner &#123; opacity: 0.88&#125;.main-inner &#123; opacity: 0.9;&#125; 使用技巧摘要写了一篇新post后，web上会默认显示全文，为了只显示部分摘要 可以在文章的md中添加 1&lt;!--more--&gt; 在任意位置截取摘要。 可以修改配置 123auto_excerpt: enable: true length: 150 来截取固定摘要。推荐第一种。 图片 使用hexo的资源文件夹，修改./_config.yml: 1post_asset_folder: true 然后new ‘xxxx’时会有一个同名文件夹 ‘xxxx’，图片example.jpg放其下。 md中用图片时有两种方式： md的语法 1![This is an example image](example.jpg) 方括号中是图片说明，图片名example.jpg前不需要加任何路径，自动相对到文件夹’xxxx’下。这样，图片可以在文章中显示，但是没法在首页显示。因为在首页的路径下，找不到这个图片 hexo的私有语法：“标签” 1&#123;% asset_img example.jpg This is an example image %&#125; asset_img声明是图片资源，同样不用加路径。这样不截断的话，可以在文章和首页同时显示图片 表格 表格无非就是 | - ： 但要注意的是：表格要与前边的内容间隔一行，不然渲染不成功 在添加或修改了一篇post后，hexo g -d 之前，最好 1hexo clean 这会清空public文件夹。如果不clean，而你又恰好修改或删除了一个tag或者category，那么例如在tag页面可能出现：“3 tags in tatol”，但是下边具体的tag只有2个]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F09%2F20%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
